{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7c7f0",
   "metadata": {},
   "source": [
    "# Step 1: Test Forward Diffusion Process\n",
    "\n",
    "Test the forward noising process q(x_t|x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpm_forward import ForwardDiffusion\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "forward_diffusion = ForwardDiffusion(\n",
    "    timesteps=1000,\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24268ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb767f7",
   "metadata": {},
   "source": [
    "# Forward Noising Grid: 6 Samples Every 150 Steps\n",
    "\n",
    "Show 6 clean digits and their progressive noising at 150-step intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1984b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 6 different MNIST images\n",
    "n_samples_forward = 6\n",
    "forward_images = torch.stack([dataset[i][0] for i in range(n_samples_forward)]).to(device)\n",
    "\n",
    "# Define timesteps to show (every 150 steps)\n",
    "forward_timestep_interval = 150\n",
    "forward_timesteps = list(range(0, 1000, forward_timestep_interval))\n",
    "\n",
    "# Store noised images at each timestep\n",
    "forward_noised_images = {}\n",
    "\n",
    "for t_val in forward_timesteps:\n",
    "    if t_val == 0:\n",
    "        forward_noised_images[t_val] = forward_images.clone().cpu()\n",
    "    else:\n",
    "        t = torch.tensor([t_val] * n_samples_forward).to(device)\n",
    "        noised = forward_diffusion.q_sample(forward_images, t)\n",
    "        forward_noised_images[t_val] = noised.cpu()\n",
    "\n",
    "print(f\"Created noised versions at timesteps: {forward_timesteps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cedd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf5b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the forward noising grid\n",
    "n_rows_forward = len(forward_timesteps)\n",
    "n_cols_forward = n_samples_forward\n",
    "\n",
    "fig, axes = plt.subplots(n_rows_forward, n_cols_forward, figsize=(n_cols_forward * 2, n_rows_forward * 2))\n",
    "\n",
    "for row_idx, t_val in enumerate(forward_timesteps):\n",
    "    for col_idx in range(n_samples_forward):\n",
    "        img = forward_noised_images[t_val][col_idx].squeeze().numpy()\n",
    "        axes[row_idx, col_idx].imshow(img, cmap='gray')\n",
    "        axes[row_idx, col_idx].axis('off')\n",
    "        \n",
    "        # Add timestep label on the left\n",
    "        if col_idx == 0:\n",
    "            axes[row_idx, col_idx].text(-0.1, 0.5, f't={t_val}', \n",
    "                                        transform=axes[row_idx, col_idx].transAxes,\n",
    "                                        fontsize=12, fontweight='bold',\n",
    "                                        va='center', ha='right')\n",
    "\n",
    "plt.suptitle(f'Forward Noising Process: {n_samples_forward} Samples Every {forward_timestep_interval} Steps', fontsize=16, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aadd1a5",
   "metadata": {},
   "source": [
    "# Step 2: create U-Net Model\n",
    "\n",
    "create the U-Net architecture that will predict noise ε_θ(x_t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f46ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_model import create_model\n",
    "\n",
    "# Create the U-Net model\n",
    "model = create_model(\n",
    "    device=device,\n",
    "    base_channels=128\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6e451",
   "metadata": {},
   "source": [
    "# Step 3: Train the Model\n",
    "\n",
    "Train the U-Net to predict noise using the simple loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f88a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_ddpm import train_epoch, p_sample, p_sample_loop, visualize_trajectory, visualize_samples\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6164581",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    avg_loss = train_epoch(model, train_loader, optimizer, forward_diffusion, device)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, EPOCHS+1), losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb253b",
   "metadata": {},
   "source": [
    "# Denoising Grid: 6 Samples Every 150 Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83080efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "# Generate 6 noise samples and denoise them step by step (every 150 timesteps)\n",
    "model.eval()\n",
    "\n",
    "n_samples = 6\n",
    "timestep_interval = 200\n",
    "\n",
    "# Start with pure noise\n",
    "x = torch.randn(n_samples, 1, 28, 28).to(device)\n",
    "\n",
    "# Store images at each interval\n",
    "timesteps_to_save = list(range(999, -1, -timestep_interval)) + [0]\n",
    "timesteps_to_save = sorted(set(timesteps_to_save), reverse=True)  # Remove duplicates and sort\n",
    "\n",
    "saved_images = {t: None for t in timesteps_to_save}\n",
    "saved_images[999] = x.clone().cpu()\n",
    "\n",
    "# Denoise step by step\n",
    "print(f\"Denoising {n_samples} samples, saving every {timestep_interval} steps...\")\n",
    "current_x = x.clone()\n",
    "\n",
    "for t_val in tqdm.tqdm(reversed(range(1000)), desc=\"Denoising\", total=1000):\n",
    "    t = torch.full((n_samples,), t_val, device=device, dtype=torch.long)\n",
    "    current_x = p_sample(model, current_x, t, forward_diffusion)\n",
    "    \n",
    "    if t_val in timesteps_to_save:\n",
    "        saved_images[t_val] = current_x.clone().cpu()\n",
    "\n",
    "# Final result\n",
    "saved_images[0] = current_x.clone().cpu()\n",
    "\n",
    "print(f\"Saved images at timesteps: {timesteps_to_save}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the denoising grid\n",
    "n_rows = len(timesteps_to_save)\n",
    "n_cols = n_samples\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "\n",
    "for row_idx, t_val in enumerate(timesteps_to_save):\n",
    "    for col_idx in range(n_samples):\n",
    "        img = saved_images[t_val][col_idx].squeeze().numpy()\n",
    "        axes[row_idx, col_idx].imshow(img, cmap='gray')\n",
    "        axes[row_idx, col_idx].axis('off')\n",
    "        \n",
    "        # Add timestep label on the left\n",
    "        if col_idx == 0:\n",
    "            axes[row_idx, col_idx].text(-0.1, 0.5, f't={t_val}', \n",
    "                                        transform=axes[row_idx, col_idx].transAxes,\n",
    "                                        fontsize=12, fontweight='bold',\n",
    "                                        va='center', ha='right')\n",
    "\n",
    "plt.suptitle(f'Denoising Process: {n_samples} Samples Every {timestep_interval} Steps', fontsize=16, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5678ed",
   "metadata": {},
   "source": [
    "# Step 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import train_classifier, evaluate_generated_samples, compare_with_real_data\n",
    "\n",
    "classifier = train_classifier(device=device, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "generated_for_eval = p_sample_loop(\n",
    "    model,\n",
    "    shape=(1000, 1, 28, 28),\n",
    "    forward_diffusion=forward_diffusion,\n",
    "    save_trajectory=False\n",
    ")\n",
    "\n",
    "print(f\"Generated {generated_for_eval.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ed859",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_generated_samples(\n",
    "    generated_images=generated_for_eval,\n",
    "    classifier=classifier,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddf870",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_confidence, baseline_confidences = compare_with_real_data(\n",
    "    classifier=classifier,\n",
    "    device=device,\n",
    "    num_samples=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Confidence distribution\n",
    "axes[0].hist(results['confidences'], bins=50, edgecolor='black', alpha=0.7, label='Generated')\n",
    "axes[0].hist(baseline_confidences, bins=50, edgecolor='red', alpha=0.5, label='Real')\n",
    "axes[0].axvline(results['avg_confidence'], color='blue', linestyle='--', linewidth=2, \n",
    "                label=f'Generated Mean: {results[\"avg_confidence\"]:.3f}')\n",
    "axes[0].axvline(baseline_confidence, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Real Mean: {baseline_confidence:.3f}')\n",
    "axes[0].set_xlabel('Classifier Confidence', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Confidence Distribution Comparison', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Class distribution\n",
    "unique, counts = np.unique(results['predictions'], return_counts=True)\n",
    "axes[1].bar(unique, counts, edgecolor='black', alpha=0.7)\n",
    "axes[1].axhline(len(results['predictions'])/10, color='red', linestyle='--', linewidth=2, \n",
    "                label='Expected (uniform)')\n",
    "axes[1].set_xlabel('Digit Class', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Distribution of Generated Digit Classes', fontsize=14)\n",
    "axes[1].set_xticks(range(10))\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Evaluation complete!\")\n",
    "print(f\"Overall Quality Score: {results['quality_score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
